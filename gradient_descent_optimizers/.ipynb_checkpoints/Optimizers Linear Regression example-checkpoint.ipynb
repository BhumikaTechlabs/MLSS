{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring gradient descent based optimizers\n",
    "\n",
    "Notebook to understand the working of various gradient descent based optimisers. We'll be exploring the simplest possible supervised learning algorithm as a use case - i.e. linear regression. It helps understand what are the shortcomings of different optimizers and how it is overcome using the newer methods.\n",
    "\n",
    "Optimizers are used to optimize the model parameters in any learning algorithm. They basically differ in what is the update step given the current parameter values and the gradient values for all parameters. Updating the parameters to minimize loss function is very similar to the real life example of a person trying to reach the bottom of a valley. At any point, you only know where you are, and the directions of maximum change (gradient) in your neighbourhood. You keep moving in direction of maximum downward slope it until you find that the slope is going upward in all your surroundings.\n",
    "\n",
    "Optimizers added:\n",
    "* Gradient Descent\n",
    "* Mini batch gradient descent\n",
    "* Momentum\n",
    "* Nesterov\n",
    "* Adagrad\n",
    "* Adadelta\n",
    "* RMSProp\n",
    "* Adam\n",
    "* Adamax\n",
    "* Nadam\n",
    "\n",
    "Author: Falak Shah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import stuff \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetically creating points around the line \n",
    "\\begin{equation}\n",
    "y = ax + b\n",
    "\\end{equation}\n",
    "by adding noise to the output. So, effectively,  \n",
    "\\begin{equation}\n",
    "y = ax + b + w, \\quad where \\\\\n",
    "w \\in N(0, \\sigma^2)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global stuff, will be used a lot down the line \n",
    "# Do not overwrite these!!\n",
    "a = 2\n",
    "b = 25\n",
    "sigma = 0.5\n",
    "\n",
    "x = np.arange(-20, 20, 0.05)                       # 40/0.05 = 800 data points\n",
    "y = a * x + b + sigma * np.random.randn(x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fadf37de810>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = [7,6])\n",
    "plt.scatter(x, y, marker='x', s=0.1)\n",
    "plt.title('Y vs X --> for line to be fit')\n",
    "plt.xlabel('X'); plt.ylabel('Y')\n",
    "plt.legend( ['y = ' + str(a) + 'x + ' + str(b) +' + noise'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is shown above. It is basically a line with some gaussian noise added. Here we know the true value of the parameters of the line, i.e. a and b. \n",
    "\n",
    "\n",
    "In a supervised learning problem setting, the task would be to determine a and b. We start with some initial pair (a,b) and moving as per the directions of the optimizer till we find the value of (a,b) minimizes the mean squared error over all data points (global minima) or a batch of data points (local minima). \n",
    "\n",
    "The loss function can be expressed as: \n",
    "\\begin{equation*}\n",
    "loss = \\frac{1}{2}\\sum_{i=1}^n(y_i - (a x_i + b) )^2 \\\\\n",
    "(a_{opt}, b_{opt}) = \\underset{a,b}{\\mathrm{argmin}} (loss)\n",
    "\\end{equation*}\n",
    "The factor of 1/2 can be discarded without any change in results but is kept for now for ease of gradients computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss for single a,b or a full grid of them \n",
    "def compute_loss(a_est, b_est):\n",
    "    return np.mean(np.square(y - (a_est * x + b_est)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tricky matrix manipulations in this cell. The main goal is to visualize the loss function for all values of a and b.\n",
    "a_explore_range = 20\n",
    "a_explore_step = 0.1\n",
    "\n",
    "# Since b has less impact on loss function, moving to larger values\n",
    "b_explore_range = 200\n",
    "b_explore_step = 1\n",
    "\n",
    "# Get full ranges of a and b\n",
    "a_est_range = np.arange(a - a_explore_range, a + a_explore_range, a_explore_step)\n",
    "b_est_range = np.arange(b - b_explore_range, b + b_explore_range, b_explore_step)\n",
    "\n",
    "# Make them into a grid. b first, as we want a to change along rows\n",
    "bgrid, agrid = np.meshgrid(b_est_range, a_est_range)\n",
    "\n",
    "# I absolutely hate for loops and I know linear algebra, hence this bit of code\n",
    "size_x = np.int(a_explore_range / a_explore_step * 2)\n",
    "size_y = np.int(b_explore_range / b_explore_step * 2)\n",
    "agrid_tiled = np.tile(np.reshape(agrid, [size_x, size_y, 1]), [1,1,x.shape[0]])\n",
    "bgrid_tiled = np.tile(np.reshape(bgrid, [size_x, size_y,1]), [1,1,x.shape[0]])\n",
    "\n",
    "# Whoosh! Loss for the full grid at once\n",
    "loss_full_grid = compute_loss(agrid_tiled, bgrid_tiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these values and move around the plot to look at it from different angles\n",
    "azimuth_angle = 25.\n",
    "elevation_angle = 35\n",
    "\n",
    "fig = plt.figure(figsize = [12,8])\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "ax.view_init(elev = elevation_angle, azim = azimuth_angle)\n",
    "ax.plot_surface(agrid, bgrid, loss_full_grid)\n",
    "ax.set_xlabel('a', fontsize=15);  ax.set_ylabel('b', fontsize=15)\n",
    "ax.set_zlabel('Loss', fontsize=15)\n",
    "ax.set_title('Visualization of the loss function', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function\n",
    "\n",
    "Observe the loss function across a grid of different parameter values. The blue curve at any point is the loss for a particular set of parameters(a,b) as shown in the figure. Notice how the decline or rise in loss is much steeper when changing **a** as compared to changing **b**. This is even more evident in the plots shown below.\n",
    "\n",
    "Some optimizers have trouble navigating along such loss functions as we'll observe. Also, this curve is obviously convex, and a very clean one at it. Do not expect such clean loss functions in real life.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [16,6])\n",
    "plt.subplot(121)\n",
    "plt.plot(a_est_range, np.mean(loss_full_grid, axis=1))\n",
    "plt.title('Mean loss values across for different a values')\n",
    "plt.subplot(122)\n",
    "plt.plot(b_est_range, np.mean(loss_full_grid, axis=0))\n",
    "plt.title('Mean loss values across for different b values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression problem mathematical formulation\n",
    "\n",
    "We are trying to fit the line \n",
    "\\begin{equation*}\n",
    "y = ax + b\n",
    "\\end{equation*}\n",
    "or in other words minimize the loss function\n",
    "\\begin{equation*}\n",
    "loss = \\frac{1}{2}\\sum_{i=1}^n(y_i - (a x_i + b) )^2 \\\\\n",
    "loss = \\frac{1}{2}\\sum_{i=1}^n (y_i^2 + (a x_i)^2 + b^2 + 2ax_ib - 2ax_iy_i - 2y_ib)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "### Gradients computation\n",
    "Computing the gradients w.r.t a and b, we have\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial loss}{\\partial a} = \\frac{1}{2}\\sum_{i=1}^n (2ax_i^2 + 2x_ib - 2x_iy_i) \\\\\n",
    "\\frac{\\partial loss}{\\partial a} = \\sum_{i=1}^n x_i (ax_i + b - y_i )\\\\\n",
    "\\frac{\\partial loss}{\\partial b} = \\sum_{i=1}^n (ax_i + b - y_i )\\\\\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions  - mostly self explanatory if you read the above ideas\n",
    "\n",
    "# Hard coded for the line - not a good idea..,but for now\n",
    "def gradient(x, y, a, b):\n",
    "    temp_err = a * x + b - y\n",
    "    loss = sum(temp_err * temp_err)\n",
    "    grad_a = sum(x * temp_err)\n",
    "    grad_b = sum(temp_err)\n",
    "    return (grad_a, grad_b, loss)\n",
    "\n",
    "# It does what the name says it does..\n",
    "def get_random_batch(x, y, batch_size):\n",
    "    i = np.random.randint(0, x.shape[0]-1, size = batch_size)\n",
    "    return(x[i], y[i])\n",
    "\n",
    "# Move these below 2 functions out of the notebook!!\n",
    "def plot_all_results(grad_store, abvalue_store, loss_store, velocity_store = None):\n",
    "    plt.figure(figsize = [20, 12])\n",
    "    plt.subplot(231)\n",
    "    plt.plot(grad_store[:,0])\n",
    "    plt.title('Gradient in a'); plt.xlabel(\"# Epochs\")\n",
    "    plt.subplot(232)\n",
    "    plt.plot(grad_store[:,1])\n",
    "    plt.title('Gradient in b'); plt.xlabel(\"# Epochs\")\n",
    "    plt.subplot(233)\n",
    "    plt.plot(abvalue_store[:,0])\n",
    "    plt.title('\"a\" values'); plt.xlabel(\"# Epochs\")\n",
    "    plt.subplot(234)\n",
    "    plt.plot(abvalue_store[:,1])\n",
    "    plt.title('\"b\" values'); plt.xlabel(\"# Epochs\")\n",
    "    plt.subplot(235)\n",
    "    plt.plot(loss_store)\n",
    "    plt.title(\"Loss\"); plt.xlabel(\"# Epochs\")\n",
    "    if velocity_store is not None:\n",
    "        plt.subplot(236)\n",
    "        plt.plot(velocity_store[:, 0])\n",
    "        plt.plot(velocity_store[:, 1], 'r*')\n",
    "        plt.legend(['Velocity of grad_a', 'Velocity of grad_b'])\n",
    "\n",
    "# Empty spaces for filling in stuff. Storing since we'll need them for plotting\n",
    "def get_data_stores(num_epochs):\n",
    "    grad_store = np.zeros([num_epochs, 2], dtype=float)\n",
    "    abvalue_store = np.zeros([num_epochs, 2], dtype=float)\n",
    "    loss_store = np.zeros([num_epochs, 1], dtype=float)\n",
    "    velocity_store = np.zeros([num_epochs, 2], dtype=float)\n",
    "    \n",
    "    return grad_store, abvalue_store, loss_store, velocity_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "* Simplest of all optimizers\n",
    "* Just one parameter - learning rate (denoted by $\\eta$)\n",
    "* Move in the direction opposite of the gradient\n",
    "* Guaranteed to converge to global optimum (convex function) and local optimum otherwise\n",
    "\n",
    "\n",
    "** Update rule ** \n",
    "\\begin{equation*}\n",
    "a = a - \\eta \\frac{\\partial(loss)}{\\partial{a}} \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "** Cons **\n",
    "* Memory intensive - loads the full dataset and computes gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "\n",
    "initial_a = 8\n",
    "initial_b = 75\n",
    "num_epochs = 1000\n",
    "eta = 8e-6\n",
    "\n",
    "grad_store, abvalue_store, loss_store, _ = get_data_stores(num_epochs)\n",
    "\n",
    "a_opt = initial_a\n",
    "b_opt = initial_b\n",
    "abvalue_store[0,:] = [a_opt, b_opt]\n",
    "\n",
    "grad_a, grad_b, loss = gradient(x, y, a_opt, b_opt)\n",
    "grad_store[0,:] = [grad_a, grad_b]\n",
    "loss_store[0] = loss\n",
    "\n",
    "for i in range(1,num_epochs):\n",
    "    a_opt = a_opt - eta * grad_a\n",
    "    b_opt = b_opt - eta * grad_b\n",
    "    abvalue_store[i,:] = [a_opt, b_opt]\n",
    "    \n",
    "    grad_a, grad_b, loss = gradient(x, y, a_opt, b_opt)\n",
    "    grad_store[i,:] = [grad_a, grad_b]\n",
    "    loss_store[i] = loss\n",
    "    \n",
    "plot_all_results(grad_store, abvalue_store, loss_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batch gradient descent\n",
    "\n",
    "This is the same as above gradient descent, except that the update is performed on a mini batch of user specified size. \n",
    "\n",
    "* Less memory intensive\n",
    "* Noisy convergence\n",
    "* More commonly used in neural nets/ problems that don't fit in memory\n",
    "\n",
    "** Update rule ** \n",
    "\\begin{equation*}\n",
    "a = a - \\eta \\frac{\\partial(loss_{batch})}{\\partial{a}} \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "where $loss_{batch}$ is the loss over the mini batch under process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini batch gradient descent\n",
    "\n",
    "initial_a = 8\n",
    "initial_b = 75\n",
    "num_epochs = 1000\n",
    "learning_rate = 5e-5\n",
    "batch_size = 128 # Lower this and watch the plots grow messy\n",
    "\n",
    "grad_store, abvalue_store, loss_store, velocity_store = get_data_stores(num_epochs)\n",
    "\n",
    "a_opt = initial_a\n",
    "b_opt = initial_b\n",
    "abvalue_store[0,:] = [a_opt, b_opt]\n",
    "\n",
    "x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "grad_a, grad_b, loss = gradient(x_batch,y_batch, initial_a, initial_b)\n",
    "grad_store[0,:] = [grad_a, grad_b]\n",
    "abvalue_store[0,:] = [a_opt, b_opt]\n",
    "loss_store[0] = loss\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    a_opt = a_opt - learning_rate * grad_a\n",
    "    b_opt = b_opt - learning_rate * grad_b\n",
    "    x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "    grad_a, grad_b, loss = gradient(x_batch, y_batch, a_opt, b_opt)\n",
    "    grad_store[i,:] = [grad_a, grad_b]\n",
    "    abvalue_store[i,:] = [a_opt, b_opt]\n",
    "    loss_store[i] = loss\n",
    "    \n",
    "plot_all_results(grad_store, abvalue_store, loss_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the noisy convergence in the above case when we used mini batches of data. Vary the batch size parameter above and observe the variations in the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "\n",
    "* To lead to faster convergence when gradient changes slowly along one of the dimensions as is the case above\n",
    "\n",
    "\\begin{equation}\n",
    "v_t = \\gamma v_{t−1} + \\eta \\frac{\\partial loss}{\\partial a} \\\\\n",
    "a = a - v_t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_a = 8\n",
    "initial_b = 75\n",
    "num_epochs = 1000\n",
    "eta = 5e-5\n",
    "batch_size = 128\n",
    "gamma = 0.9\n",
    "\n",
    "grad_store, abvalue_store, loss_store, velocity_store = get_data_stores(num_epochs)\n",
    "\n",
    "a_opt = initial_a\n",
    "b_opt = initial_b\n",
    "abvalue_store[0,:] = [a_opt, b_opt]\n",
    "\n",
    "\n",
    "x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "grad_a, grad_b, loss = gradient(x_batch,y_batch, initial_a, initial_b)\n",
    "grad_store[0,:] = [grad_a, grad_b]\n",
    "velocity_a = eta*grad_a\n",
    "velocity_b = eta*grad_b\n",
    "velocity_store[0,:] = [velocity_a, velocity_b]\n",
    "abvalue_store[0,:] = [a_opt, b_opt]\n",
    "loss_store[0] = loss\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    a_opt = a_opt - velocity_a\n",
    "    b_opt = b_opt - velocity_b\n",
    "    x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "    grad_a, grad_b, loss = gradient(x_batch, y_batch, a_opt, b_opt)\n",
    "    velocity_a = gamma* velocity_a + eta * grad_a\n",
    "    velocity_b = gamma* velocity_b + eta * grad_b\n",
    "    grad_store[i,:] = [grad_a, grad_b]\n",
    "    abvalue_store[i,:] = [a_opt, b_opt]\n",
    "    velocity_store[i,:] = [velocity_a, velocity_b]\n",
    "    loss_store[i] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_results(grad_store, abvalue_store, loss_store, velocity_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov\n",
    "\n",
    "* To prevent the ball from sliding up the other slope with all the momentum it gained while coming down\n",
    "\n",
    "\\begin{equation}\n",
    "v_t = \\gamma v_{t−1} + \\eta \\frac{\\partial loss(a - \\gamma v_{t-1})}{\\partial a }\\\\\n",
    "a = a - v_t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_a = 8\n",
    "initial_b = 75\n",
    "num_epochs = 1000\n",
    "eta = 5e-5\n",
    "batch_size = 128\n",
    "gamma = 0.9\n",
    "\n",
    "grad_store, abvalue_store, loss_store, velocity_store = get_data_stores(num_epochs)\n",
    "\n",
    "a_opt = initial_a\n",
    "b_opt = initial_b\n",
    "abvalue_store[0,:] = [a_opt, b_opt]\n",
    "\n",
    "\n",
    "x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "grad_a, grad_b, loss = gradient(x_batch,y_batch, initial_a, initial_b)\n",
    "grad_store[0,:] = [grad_a, grad_b]\n",
    "velocity_a = eta * grad_a\n",
    "velocity_b = eta * grad_b\n",
    "velocity_store[0,:] = [velocity_a, velocity_b]\n",
    "abvalue_store[0,:] = [a_opt, b_opt]\n",
    "loss_store[0] = loss\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    a_opt = a_opt - velocity_a\n",
    "    b_opt = b_opt - velocity_b\n",
    "    x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "    grad_a, grad_b, loss = gradient(x_batch, y_batch, a_opt - gamma * velocity_a, b_opt - gamma * velocity_b)\n",
    "    velocity_a = gamma * velocity_a + eta * grad_a\n",
    "    velocity_b = gamma * velocity_b + eta * grad_b\n",
    "    grad_store[i,:] = [grad_a, grad_b]\n",
    "    abvalue_store[i,:] = [a_opt, b_opt]\n",
    "    velocity_store[i,:] = [velocity_a, velocity_b]\n",
    "    loss_store[i] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_results(grad_store, abvalue_store, loss_store, velocity_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad\n",
    "\\begin{equation*}\n",
    "a_{t+1,i} = a_{t,i} −  \\frac{\\eta }{G_{t,ii} + \\epsilon} \\cdot g_{t,i} \n",
    "\\end{equation*}\n",
    "where $G_t \\in R^{d×d}$ is a diagonal matrix where each diagonal element i, i is the sum of the squares of the\n",
    "gradients w.r.t. the parameter up to time step $t^{i}$, while $\\epsilon$ is a smoothing term that avoids division by zero (usually\n",
    "on the order of 1e − 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_a = 8\n",
    "initial_b = 75\n",
    "num_epochs = 3000 # It shouldn't take these many ideally - check b again\n",
    "eta = 9e-1  # Keep this one high for adagrad...\n",
    "batch_size = 128\n",
    "eps = 1e-8\n",
    "\n",
    "grad_store, abvalue_store, loss_store, _ = get_data_stores(num_epochs)\n",
    "\n",
    "a_opt = initial_a\n",
    "b_opt = initial_b\n",
    "abvalue_store[0,:] = [a_opt, b_opt]\n",
    "\n",
    "\n",
    "x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "grad_a, grad_b, loss = gradient(x_batch,y_batch, initial_a, initial_b)\n",
    "grad_store[0,:] = [grad_a, grad_b]\n",
    "abvalue_store[0,:] = [a_opt, b_opt]\n",
    "loss_store[0] = loss\n",
    "\n",
    "for i in range(1,num_epochs):\n",
    "    a_opt = a_opt - eta * grad_a / np.sqrt(sum(np.power(grad_store[:i,0],2)) + eps)\n",
    "    b_opt = b_opt - eta * grad_b / np.sqrt(sum(np.power(grad_store[:i,1],2)) + eps)\n",
    "    x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "    grad_a, grad_b, loss = gradient(x_batch, y_batch, a_opt , b_opt)\n",
    "    grad_store[i,:] = [grad_a, grad_b]\n",
    "    abvalue_store[i,:] = [a_opt, b_opt]\n",
    "    loss_store[i] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_results(grad_store, abvalue_store, loss_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Adadelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, a, b):\n",
    "    temp_err = a * x + b - y\n",
    "    loss = sum(temp_err * temp_err)\n",
    "    grad_a = sum(x * temp_err)\n",
    "    grad_b = sum(temp_err)\n",
    "    return (grad_a, grad_b, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "initial_a = 8\n",
    "initial_b = 75\n",
    "num_epochs = 3000\n",
    "epsilon = 1e-8\n",
    "initial_theta = np.array([initial_a, initial_b])\n",
    "gamma = 0.999\n",
    "eta = 8e-2\n",
    "batch_size = 128\n",
    "\n",
    "grad_store, abvalue_store, loss_store, _ = get_data_stores(num_epochs)\n",
    "\n",
    "theta = initial_theta\n",
    "\n",
    "x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "grad_a, grad_b, loss = gradient(x_batch,y_batch, initial_a, initial_b)\n",
    "grad = np.array([grad_a, grad_b])\n",
    "\n",
    "expected_grad = np.square(grad)\n",
    "\n",
    "grad_store[0,:] = list(grad)\n",
    "abvalue_store[0,:] = list(initial_theta)\n",
    "loss_store[0,:] = loss\n",
    "\n",
    "for i in range(1,num_epochs):\n",
    "    #Basic idea of gradient \n",
    "    RMS_grad = np.sqrt(expected_grad + epsilon)\n",
    "    delta_theta = -(eta/RMS_grad) * grad\n",
    "    theta = theta + delta_theta\n",
    "    #Update expected_grad\n",
    "    x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "    grad_a, grad_b, loss = gradient(x_batch,y_batch, theta[0], theta[1])\n",
    "    grad = np.array([grad_a, grad_b])\n",
    "    expected_grad = gamma * expected_grad + (1 - gamma) * np.square(grad)\n",
    "    \n",
    "    grad_store[i,:] = list(grad)\n",
    "    abvalue_store[i,:] = list(theta)\n",
    "    loss_store[i] = loss\n",
    "    \n",
    "    #print grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_all_results(grad_store, abvalue_store, loss_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parameter's units correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "initial_a = 8\n",
    "initial_b = 75\n",
    "num_epochs = 2000\n",
    "epsilon = 1e-8\n",
    "initial_theta = np.array([initial_a, initial_b])\n",
    "gamma = 0.9\n",
    "eta = 1e-1\n",
    "batch_size = 128\n",
    "\n",
    "grad_store, abvalue_store, loss_store, _ = get_data_stores(num_epochs)\n",
    "\n",
    "theta = initial_theta\n",
    "\n",
    "x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "grad_a, grad_b, loss = gradient(x_batch,y_batch, initial_a, initial_b)\n",
    "grad = np.array([grad_a, grad_b])\n",
    "\n",
    "expected_grad = np.square(grad)\n",
    "delta_theta = -(eta/np.sqrt(expected_grad + epsilon)) * grad                                                                                                                                                                \n",
    "expected_delta_theta = np.square(delta_theta)\n",
    "\n",
    "grad_store[0,:] = list(grad)\n",
    "abvalue_store[0,:] = list(initial_theta)\n",
    "loss_store[0,:] = loss\n",
    "\n",
    "theta = theta + delta_theta\n",
    "x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "grad_a, grad_b, loss = gradient(x_batch,y_batch, theta[0], theta[1])\n",
    "grad = np.array([grad_a, grad_b])\n",
    "expected_grad = gamma * expected_grad + (1 - gamma) * np.square(grad)\n",
    "\n",
    "grad_store[1,:] = list(grad)\n",
    "abvalue_store[1,:] = list(initial_theta)\n",
    "loss_store[1,:] = loss\n",
    "\n",
    "for i in range(2,num_epochs):\n",
    "    #Basic idea of gradient \n",
    "    rms_delta_theta = np.sqrt(expected_delta_theta + epsilon)\n",
    "    rms_grad = np.sqrt(expected_grad + epsilon)\n",
    "    delta_theta = - grad * (rms_delta_theta /(rms_grad))\n",
    "    \n",
    "    theta = theta + delta_theta\n",
    "    \n",
    "    #theta = theta + expected_delta_theta\n",
    "    #Update expected_grad\n",
    "    x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "    grad_a, grad_b, loss = gradient(x_batch,y_batch, theta[0], theta[1])\n",
    "    \n",
    "    grad = np.array([grad_a, grad_b])\n",
    "    \n",
    "    expected_grad = gamma * expected_grad + (1 - gamma) * np.square(grad)\n",
    "    expected_delta_theta = gamma * expected_delta_theta + (1-gamma) * np.square(delta_theta)\n",
    "    \n",
    "    grad_store[i,:] = list(grad)\n",
    "    abvalue_store[i,:] = list(theta)\n",
    "    loss_store[i] = loss\n",
    "    \n",
    "    #print grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_results(grad_store, abvalue_store, loss_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\\begin{equation}\n",
    "m_t = \\beta_1 m_{t−1} + (1 − \\beta_1)g_t \\\\\n",
    "v_t = \\beta_2v_{t−1} + (1 − \\beta_2)g^2_t \\\\\n",
    "\\hat{m_t} = \\frac{m_t}{1 − β_t}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "initial_a = 8\n",
    "initial_b = 75\n",
    "num_epochs = 2000\n",
    "epsilon = 1e-8\n",
    "\n",
    "theta = np.array([initial_a, initial_b])\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "batch_size = 1024\n",
    "eta = 1e-1\n",
    "grad_store, abvalue_store, loss_store, velocity_store = get_data_stores(num_epochs)\n",
    "\n",
    "\n",
    "mt = np.array([0, 0])\n",
    "vt = np.array([0, 0])\n",
    "\n",
    "for i in range(num_epochs): \n",
    "    t = i+1\n",
    "    #Update expected_grad\n",
    "    x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "    \n",
    "    grad_a, grad_b, loss = gradient(x_batch,y_batch, theta[0], theta[1])\n",
    "    grad = np.array([grad_a, grad_b])\n",
    "    \n",
    "    mt = (beta1 * mt + (1 - beta1) * grad)\n",
    "    vt = (beta2 * vt + (1 - beta2) * (grad ** 2))\n",
    "    \n",
    "    mhat = mt / (1 - beta1 ** t ) \n",
    "    vhat = vt / (1 - beta2 ** t )\n",
    "    \n",
    "    grad_store[i,:] = list(grad)\n",
    "    abvalue_store[i,:] = list(theta)\n",
    "    loss_store[i] = loss\n",
    "    \n",
    "#     print vhat\n",
    "    \n",
    "    #Basic idea of gradient \n",
    "    delta_theta = -1 * eta * mhat/(np.sqrt(vhat) + epsilon)\n",
    "    theta = theta + delta_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_results(grad_store, abvalue_store, loss_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaMax\n",
    "\n",
    "\\begin{equation}\n",
    "u_t = \\beta^\\infty_2 v_{t−1} + (1 − β_2^\\infty)|g_t|^\\infty\\\\\n",
    "u_t = max(\\beta_2 \\cdot v_{t−1}, |g_t|) \\\\\n",
    "\\theta_{t+1} = \\theta_t − \\eta \\frac{\\hat{m_t}}{u_t}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "initial_a = 8\n",
    "initial_b = 75\n",
    "num_epochs = 2000\n",
    "epsilon = 1e-8\n",
    "\n",
    "theta = np.array([initial_a, initial_b])\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "batch_size = 1024\n",
    "eta = 1e-1\n",
    "grad_store, abvalue_store, loss_store, velocity_store = get_data_stores(num_epochs)\n",
    "\n",
    "\n",
    "mt = np.array([0, 0])\n",
    "vt = np.array([0, 0])\n",
    "\n",
    "for i in range(num_epochs): \n",
    "    t = i+1\n",
    "    #Update expected_grad\n",
    "    x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "    \n",
    "    grad_a, grad_b, loss = gradient(x_batch,y_batch, theta[0], theta[1])\n",
    "    grad = np.array([grad_a, grad_b])\n",
    "    \n",
    "    mt = (beta1 * mt + (1 - beta1) * grad)\n",
    "    vt = (beta2 * vt + (1 - beta2) * (grad ** 2))\n",
    "    \n",
    "    ut = np.max(np.append(beta2 * vt, grad, axis=0), axis=0)\n",
    "    \n",
    "    mhat = mt / (1 - beta1 ** t ) \n",
    "#     vhat = vt / (1 - beta2 ** t )\n",
    "    \n",
    "    grad_store[i,:] = list(grad)\n",
    "    abvalue_store[i,:] = list(theta)\n",
    "    loss_store[i] = loss\n",
    "    \n",
    "#     print vhat\n",
    "    \n",
    "    #Basic idea of gradient \n",
    "    delta_theta = -1 * eta * mhat/(np.sqrt(ut) + epsilon)\n",
    "    theta = theta + delta_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_results(grad_store, abvalue_store, loss_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nadam\n",
    "\\begin{equation}\n",
    "    \\theta_{t+1} = \\theta_{t} − \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} (\\beta_1 \\hat{m_t} + \\frac{(1-\\beta_1)g_tt}{1-\\beta_1^t})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "initial_a = 8\n",
    "initial_b = 75\n",
    "num_epochs = 2000\n",
    "epsilon = 1e-8\n",
    "\n",
    "theta = np.array([initial_a, initial_b])\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "batch_size = 1024\n",
    "eta = 1e-1\n",
    "grad_store, abvalue_store, loss_store, velocity_store = get_data_stores(num_epochs)\n",
    "\n",
    "\n",
    "mt = np.array([0, 0])\n",
    "vt = np.array([0, 0])\n",
    "\n",
    "for i in range(num_epochs): \n",
    "    t = i+1\n",
    "    #Update expected_grad\n",
    "    x_batch, y_batch = get_random_batch(x,y, batch_size)\n",
    "    \n",
    "    grad_a, grad_b, loss = gradient(x_batch,y_batch, theta[0], theta[1])\n",
    "    grad = np.array([grad_a, grad_b])\n",
    "    \n",
    "    mt = (beta1 * mt + (1 - beta1) * grad)\n",
    "    vt = (beta2 * vt + (1 - beta2) * (grad ** 2))\n",
    "    \n",
    "    ut = np.max(np.append(beta2 * vt, grad, axis=0), axis=0)\n",
    "    \n",
    "    mhat = mt / (1 - beta1 ** t ) \n",
    "    vhat = vt / (1 - beta2 ** t )\n",
    "    \n",
    "    grad_store[i,:] = list(grad)\n",
    "    abvalue_store[i,:] = list(theta)\n",
    "    loss_store[i] = loss\n",
    "    \n",
    "#     print vhat\n",
    "    \n",
    "    #Basic idea of gradient \n",
    "    delta_theta = -1 * eta * (beta1 * mhat + (1 - beta1)/(1 - beta1 ** t) * grad)/(np.sqrt(ut) + epsilon)\n",
    "    theta = theta + delta_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_results(grad_store, abvalue_store, loss_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
