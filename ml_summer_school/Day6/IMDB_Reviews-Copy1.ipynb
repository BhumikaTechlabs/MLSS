{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the dataset on the machine\n",
    "train_pos_folder = \"/home/bhumika/Documents/ss18/ml_summer_school/Day6/IMDB_SummerSchool/train/pos/\"\n",
    "train_neg_folder = \"/home/bhumika/Documents/ss18/ml_summer_school/Day6/IMDB_SummerSchool/train/neg/\"\n",
    "train_unsup_folder = \"/home/bhumika/Documents/ss18/ml_summer_school/Day6/IMDB_SummerSchool/train/unsup/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset files - just a utility code\n",
    "train_pos_files = os.listdir(train_pos_folder)\n",
    "print len(train_pos_files)\n",
    "train_neg_files = os.listdir(train_neg_folder)\n",
    "print len(train_neg_files)\n",
    "train_unsup_files = os.listdir(train_unsup_folder)\n",
    "print len(train_unsup_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file/review properties\n",
    "train_pos_df = pd.DataFrame(columns=[\"Unique ID\",\"File Name\",\"File ID\",\"Rating\",\"Review\"])\n",
    "#%%time\n",
    "dataset = \"train_pos_\"\n",
    "for f in train_pos_files:\n",
    "    open_f = open(train_pos_folder+f)\n",
    "    review = (open_f.read()).replace(\"<br />\",\"\\n\")\n",
    "    #print review\n",
    "    file_name = f\n",
    "    file_id = int(f.split(\"_\")[0])\n",
    "    rating = int((f.split(\"_\")[1]).split(\".\")[0])\n",
    "    unique_id = dataset + str(file_id) \n",
    "    train_pos_df.loc[len(train_pos_df)] = [unique_id,file_name,file_id,rating,review]\n",
    "    \n",
    "train_neg_df = pd.DataFrame(columns=[\"Unique ID\",\"File Name\",\"File ID\",\"Rating\",\"Review\"])\n",
    "#%%time\n",
    "dataset = \"train_neg_\"\n",
    "for f in train_neg_files:\n",
    "    open_f = open(train_neg_folder+f)\n",
    "    review = (open_f.read()).replace(\"<br />\",\"\\n\")\n",
    "    #print review\n",
    "    file_name = f\n",
    "    file_id = int(f.split(\"_\")[0])\n",
    "    rating = int((f.split(\"_\")[1]).split(\".\")[0])\n",
    "    unique_id = dataset + str(file_id) \n",
    "    train_neg_df.loc[len(train_neg_df)] = [unique_id,file_name,file_id,rating,review]\n",
    "    \n",
    "train_df = pd.concat([train_pos_df, train_neg_df],ignore_index=True)\n",
    "\n",
    "# Label the reviews according to the rating\n",
    "train_df[\"Label\"] = \"neg\"\n",
    "train_df.loc[train_df[\"Rating\"] >= 5, \"Label\"] = \"pos\"\n",
    "\n",
    "# Check groups - just a utility code\n",
    "train_df.groupby(\"Label\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the review\n",
    "import re, string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "import pdb\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def tokenize_text(txt):\n",
    "    \n",
    "    if len(txt.split()) < 10:\n",
    "            return []\n",
    "    txt = re.sub(r\"[^A-Za-z]\", \" \", txt)\n",
    "    txt = re.sub(r\"\\\"\", \" \\\" \", txt)\n",
    "    txt = re.sub(r\",\", \" , \", txt)\n",
    "    txt = re.sub(r\"\\(\", \" ( \", txt)\n",
    "    txt = re.sub(r\"\\)\", \" ) \", txt)\n",
    "    txt = re.sub(r\"\\s{2,}\", \" \", txt)\n",
    "    \n",
    "    tokens = word_tokenize(txt)\n",
    "    \n",
    "    #tokens.append('length_' + str(len(tokens) / 10))\n",
    "    \n",
    "    tokens = [stemmer.stem(tok) for tok in tokens if (len(tok) > 2) or (tok in string.punctuation)]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = a review ; y = the corressponding predicted review category, pos/neg\n",
    "X_train = train_df[\"Review\"]\n",
    "y_train = train_df[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold CV\n",
    "X_train_df = train_df.copy(deep=True)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pdb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df_errors_dfs = []\n",
    "\n",
    "avg_precision = 0\n",
    "avg_recall = 0\n",
    "avg_f1 = 0\n",
    "\n",
    "# Vectorize the tokens!\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, tokenizer=tokenize_text, ngram_range=(1, 3))\n",
    "# Generate the train/validation sets\n",
    "kf = KFold(n_splits = 5)\n",
    "for train_inds, test_inds in kf.split(X_train_df):\n",
    "    \n",
    "    cv_train_df = X_train_df.iloc[train_inds, :]\n",
    "    cv_test_df = X_train_df.iloc[test_inds, :]\n",
    "    \n",
    "    train_sents = cv_train_df['Review']\n",
    "    train_labels = cv_train_df['Label']\n",
    "    test_ids = cv_test_df['Unique ID']\n",
    "    test_sents = cv_test_df['Review']\n",
    "    test_labels = cv_test_df['Label']\n",
    "    \n",
    "    # Select the classifier\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=5, random_state=42)\n",
    "    # Put it into the pipeline finally, for vectorization & classification !\n",
    "    pipe = Pipeline([('vectorizer', vectorizer), ('clf', clf)])\n",
    "    # Fit onto the (k-1) training sets\n",
    "    %time pipe.fit(train_sents, train_labels)\n",
    "    # Predict on the k_th validation set\n",
    "    %time test_preds = pipe.predict(test_sents)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    avg_precision += precision_score(test_labels, test_preds, pos_label = 'pos')\n",
    "    avg_recall += recall_score(test_labels, test_preds, pos_label = 'pos')\n",
    "    avg_f1 += f1_score(test_labels, test_preds, pos_label = 'pos')\n",
    "\n",
    "    df_errors_subset = pd.DataFrame()\n",
    "    df_errors_subset['Unique ID'] = test_ids\n",
    "    df_errors_subset['Review'] = test_sents\n",
    "    df_errors_subset['Label'] = test_labels\n",
    "    df_errors_subset['Model Prediction'] = test_preds\n",
    "    #df_errors_subset['Prediction Probability'] = test_preds_scores\n",
    "    \n",
    "    df_errors_subset = df_errors_subset[df_errors_subset['Label'] != df_errors_subset['Model Prediction']]\n",
    "    df_errors_dfs.append(df_errors_subset)\n",
    "    \n",
    "    print 25*'-'\n",
    "    \n",
    "df_errors_train = pd.concat(df_errors_dfs)\n",
    "\n",
    "print \"Average precision: \" + str(avg_precision / 5.0)\n",
    "print \"Average recall: \" + str(avg_recall / 5.0)\n",
    "print \"Average f1: \" + str(avg_f1 / 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
